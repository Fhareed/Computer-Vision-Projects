# -*- coding: utf-8 -*-
"""MLCompVision Assigment1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tV9ZwGKoAQc95_d4YXfzJk_8qthEuUvI
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/ColabNotebooks/CIFAR10-mini.zip

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.models import Model
from keras.utils import to_categorical
from PIL import Image

import zipfile
import os
zip_ref = zipfile.ZipFile("/content/drive/MyDrive/ColabNotebooks/CIFAR10-mini.zip", 'r')
zip_ref.extractall("/tmp")
zip_ref.close()
def inspect_zipfile(zip_filepath):
    # Extract the zip file
    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:
        zip_ref.extractall('CIFAR10-mini')

    # Define paths to training and testing folders
    training_folder = os.path.join('CIFAR10-mini', 'training')
    testing_folder = os.path.join('CIFAR10-mini', 'testing')

    # Get the list of files in each folder
    training_files = os.listdir(training_folder)
    testing_files = os.listdir(testing_folder)

    # Count the number of files
    num_training_files = len(training_files)
    num_testing_files = len(testing_files)

    print(f"Number of files in training folder: {num_training_files}")
    print(f"Number of files in testing folder: {num_testing_files}")

    # Optionally, list the first few files for inspection
    print(f"Sample files in training folder: {training_files[:5]}")
    print(f"Sample files in testing folder: {testing_files[:5]}")

# Call the function with your CIFAR10-mini.zip file path
inspect_zipfile('/content/drive/MyDrive/ColabNotebooks/CIFAR10-mini.zip')

os.listdir('CIFAR10-mini/testing')

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from PIL import Image
import os

# Load labels from a text file
def load_labels(filename):
    with open(filename, 'r') as file:
        lines = file.readlines()
    label_count = len(lines)
    labels = np.empty((label_count, 1), dtype='int')
    for i, line in enumerate(lines):
        labels[i] = int(line.strip())
    return labels

# Load images from a folder
def load_images(folder, image_count, image_size):
    array_shape = (image_count, image_size[0], image_size[1], image_size[2])
    imageset = np.empty(array_shape, dtype='float')
    for i in range(image_count):
        image = Image.open(os.path.join(folder, f'image_{i:04d}.png'))
        imageset[i] = np.asarray(image)
    return imageset

# Normalize the dataset
def normalize_dataset(images):
    return (images / 127.5) - 1

# # Split the test dataset into test and validation sets
# def split_test_val(X, Y, split_point):
#     X_val = X[:split_point]
#     Y_val = Y[:split_point]
#     X_test = X[split_point:]
#     Y_test = Y[split_point:]
#     return X_val, Y_val, X_test, Y_test



def split_test_val(X, Y, split_point):
    """
    Splits the dataset into validation and test sets.

    Parameters:
    X (np.ndarray): Features dataset to be split.
    Y (np.ndarray): Labels dataset to be split.
    split_point (int): The index at which to split the datasets.

    Returns:
    tuple: Four arrays containing the validation and test data.
    """
    # Validate the split point
    if split_point < 0 or split_point > len(X) or split_point > len(Y):
        raise ValueError("split_point must be between 0 and the length of both X and Y")
    
    # Split the datasets
    X_val = X[:split_point]
    Y_val = Y[:split_point]
    X_test = X[split_point:]
    Y_test = Y[split_point:]

    return X_val, Y_val, X_test, Y_test

# Example usage
# X = np.array([...])  # Your feature dataset here
# Y = np.array([...])  # Your label dataset here
# X_val, Y_val, X_test, Y_test = split_test_val(X, Y, 3000)


# One-hot encode labels
def one_hot_encode_labels(labels, num_classes=10):
    return to_categorical(labels, num_classes=num_classes)

# Create the neural network model
def create_model(input_shape):
    x = Input(shape=input_shape)
    y = Conv2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(x)
    y = MaxPooling2D()(y)
    y = Conv2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = MaxPooling2D()(y)
    y = Conv2D(filters=128, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = MaxPooling2D()(y)
    y = Flatten()(y)
    y = Dense(128, activation='relu')(y)
    y = Dense(64, activation='relu')(y)
    y = Dense(10, activation='softmax')(y)

    model = Model(inputs=x, outputs=y)
    return model

# Compile the model
def compile_model(model):
    model.compile(optimizer=Adam(), 
                  loss='categorical_crossentropy', 
                  metrics=['accuracy'])

# Set the parameters
image_size = (32, 32, 3)
num_train_images = 10000
num_test_images = 10000
split_point = 3000

# Load and preprocess the datasets
X_train = load_images('/content/CIFAR10-mini/training', num_train_images, image_size)
#Y_train = load_labels('/content/drive/MyDrive/ColabNotebooks/CIFAR10-mini/training/labels.txt')
Y_train = load_labels('/content/CIFAR10-mini/training/labels.csv')
X_test = load_images('/content/CIFAR10-mini/testing', num_test_images, image_size)
Y_test = load_labels('/content/CIFAR10-mini/testing/labels.csv')

# Normalize the datasets
X_train = normalize_dataset(X_train)
X_test = normalize_dataset(X_test)

# Split the test data into validation and test sets
X_val, Y_val, X_test_remaining, Y_test_remaining = split_test_val(X_test, Y_test, split_point)

# One-hot encode the labels
Y_train_encoded = one_hot_encode_labels(Y_train, num_classes=10)
Y_val_encoded = one_hot_encode_labels(Y_val, num_classes=10)
Y_test_encoded = one_hot_encode_labels(Y_test_remaining, num_classes=10)

# Create and compile the model
input_shape = (32, 32, 3)
model = create_model(input_shape)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model using the loaded dataset for 12 epochs
history = model.fit(X_train, Y_train_encoded,
                    validation_data=(X_val, Y_val_encoded),
                    epochs=12,
                    batch_size=32)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_remaining, Y_test_encoded)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

