# -*- coding: utf-8 -*-
"""MLCompVision Assigment1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tV9ZwGKoAQc95_d4YXfzJk_8qthEuUvI
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import keras.backend as K


def get_centralized_gradients(optimizer, loss, params):
    """Compute the centralized gradients.

    This function is ideally not meant to be used directly unless you are building a custom optimizer, in which case you
    could point `get_gradients` to this function. This is a modified version of
    `tf.keras.optimizers.Optimizer.get_gradients`.

    # Arguments:
        optimizer: a `tf.keras.optimizers.Optimizer object`. The optimizer you are using.
        loss: Scalar tensor to minimize.
        params: List of variables.

    # Returns:
      A gradients tensor.

    # Reference:
        [Yong et al., 2020](https://arxiv.org/abs/2004.01461)
    """

    # We here just provide a modified get_gradients() function since we are trying to just compute the centralized
    # gradients at this stage which can be used in other optimizers.
    grads = []
    for grad in K.gradients(loss, params):
        grad_len = len(grad.shape)
        if grad_len > 1:
            axis = list(range(grad_len - 1))
            grad -= tf.reduce_mean(grad,
                                   axis=axis,
                                   keep_dims=True)
        grads.append(grad)

    if None in grads:
        raise ValueError('An operation has `None` for gradient. '
                         'Please make sure that all of your ops have a '
                         'gradient defined (i.e. are differentiable). '
                         'Common ops without gradient: '
                         'K.argmax, K.round, K.eval.')
    if hasattr(optimizer, 'clipnorm') and optimizer.clipnorm > 0:
        norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))
        grads = [
            tf.keras.optimizers.clip_norm(
                g,
                optimizer.clipnorm,
                norm) for g in grads]
    if hasattr(optimizer, 'clipvalue') and optimizer.clipvalue > 0:
        grads = [K.clip(g, -optimizer.clipvalue, optimizer.clipvalue)
                 for g in grads]
    return grads


def centralized_gradients_for_optimizer(optimizer):
    """Create a centralized gradients functions for a specified optimizer.

    # Arguments:
        optimizer: a `tf.keras.optimizers.Optimizer object`. The optimizer you are using.

    # Usage:

    ```py
    >>> opt = tf.keras.optimizers.Adam(learning_rate=0.1)
    >>> opt.get_gradients = gctf.centralized_gradients_for_optimizer(opt)
    >>> model.compile(optimizer = opt, ...)
    ```
    """

    def get_centralized_gradients_for_optimizer(loss, params):
        return get_centralized_gradients(optimizer, loss, params)

    return get_centralized_gradients_for_optimizer

import tensorflow as tf
import keras.backend as K



def update_optimizer(optimizer):
    optimizer.get_gradients = centralized_gradients_for_optimizer(optimizer)
    return optimizer

def adagrad(learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07):
    optimizer = tf.keras.optimizers.Adagrad(
        learning_rate=learning_rate,
        initial_accumulator_value=initial_accumulator_value,
        epsilon=epsilon)
    return update_optimizer(optimizer)


def adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07):
    optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate,
                                             rho=rho,
                                             epsilon=epsilon)
    return update_optimizer(optimizer)


def adam(
        learning_rate=0.001,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-7,
        amsgrad=False):
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=learning_rate,
        beta_1=beta_1,
        beta_2=beta_2,
        epsilon=epsilon,
        amsgrad=amsgrad)
    return update_optimizer(optimizer)


def adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07):
    optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate,
                                           beta_1=beta_1,
                                           beta_2=beta_2,
                                           epsilon=epsilon)
    return update_optimizer(optimizer)


def ftrl(
        learning_rate=0.001,
        learning_rate_power=-0.5,
        initial_accumulator_value=0.1,
        l1_regularization_strength=0.0,
        l2_regularization_strength=0.0,
        l2_shrinkage_regularization_strength=0.0,
        beta=0.0):
    optimizer = tf.keras.optimizers.Adamax(
        learning_rate=learning_rate,
        learning_rate_power=learning_rate_power,
        initial_accumulator_value=initial_accumulator_value,
        l1_regularization_strength=l1_regularization_strength,
        l2_regularization_strength=l2_regularization_strength,
        l2_shrinkage_regularization_strength=l2_shrinkage_regularization_strength,
        beta=beta)
    return update_optimizer(optimizer)


def nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07):
    optimizer = tf.keras.optimizers.Nadam(
        learning_rate=learning_rate,
        beta_1=beta_1,
        beta_2=beta_2,
        epsilon=epsilon)
    return update_optimizer(optimizer)


def rmsprop(
        learning_rate=0.001,
        rho=0.9,
        momentum=0.0,
        epsilon=1e-07,
        centered=False):
    optimizer = tf.keras.optimizers.RMSprop(
        learning_rate=learning_rate,
        rho=rho,
        momentum=momentum,
        epsilon=epsilon,
        centered=centered)
    return update_optimizer(optimizer)


def sgd(learning_rate=0.01, momentum=0.0, nesterov=False):
    optimizer = tf.keras.optimizers.SGD(
        learning_rate=learning_rate,
        momentum=momentum,
        nesterov=nesterov)
    return update_optimizer(optimizer)

!unzip /content/drive/MyDrive/ColabNotebooks/CIFAR10-mini.zip

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.models import Model
from keras.utils import to_categorical
from PIL import Image

import zipfile
import os
zip_ref = zipfile.ZipFile("/content/drive/MyDrive/ColabNotebooks/CIFAR10-mini.zip", 'r')
zip_ref.extractall("/tmp")
zip_ref.close()
def inspect_zipfile(zip_filepath):
    # Extract the zip file
    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:
        zip_ref.extractall('CIFAR10-mini')

    # Define paths to training and testing folders
    training_folder = os.path.join('CIFAR10-mini', 'training')
    testing_folder = os.path.join('CIFAR10-mini', 'testing')

    # Get the list of files in each folder
    training_files = os.listdir(training_folder)
    testing_files = os.listdir(testing_folder)

    # Count the number of files
    num_training_files = len(training_files)
    num_testing_files = len(testing_files)

    print(f"Number of files in training folder: {num_training_files}")
    print(f"Number of files in testing folder: {num_testing_files}")

    # Optionally, list the first few files for inspection
    print(f"Sample files in training folder: {training_files[:5]}")
    print(f"Sample files in testing folder: {testing_files[:5]}")

# Call the function with your CIFAR10-mini.zip file path
inspect_zipfile('/content/drive/MyDrive/ColabNotebooks/CIFAR10-mini.zip')

os.listdir('CIFAR10-mini/testing')

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from PIL import Image
import os

# Load labels from a text file
def load_labels(filename):
    with open(filename, 'r') as file:
        lines = file.readlines()
    label_count = len(lines)
    labels = np.empty((label_count, 1), dtype='int')
    for i, line in enumerate(lines):
        labels[i] = int(line.strip())
    return labels

# Load images from a folder
def load_images(folder, image_count, image_size):
    array_shape = (image_count, image_size[0], image_size[1], image_size[2])
    imageset = np.empty(array_shape, dtype='float')
    for i in range(image_count):
        image = Image.open(os.path.join(folder, f'image_{i:04d}.png'))
        imageset[i] = np.asarray(image)
    return imageset

# Normalize the dataset
def normalize_dataset(images):
    return (images / 127.5) - 1

# Split the test dataset into test and validation sets
def split_test_val(X, Y, split_point):
    X_val = X[:split_point]
    Y_val = Y[:split_point]
    X_test = X[split_point:]
    Y_test = Y[split_point:]
    return X_val, Y_val, X_test, Y_test

# One-hot encode labels
def one_hot_encode_labels(labels, num_classes=10):
    return to_categorical(labels, num_classes=num_classes)

# Create the neural network model
def create_model(input_shape):
    x = Input(shape=input_shape)
    y = Conv2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(x)
    y = MaxPooling2D()(y)
    y = Conv2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = MaxPooling2D()(y)
    y = Conv2D(filters=128, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = Flatten()(y)
    y = Dense(128, activation='relu')(y)
    y = Dense(128, activation='relu')(y)
    y = Dense(10, activation='softmax')(y)

    model = Model(inputs=x, outputs=y)
    return model

# Set the parameters
image_size = (32, 32, 3)
num_train_images = 10000
num_test_images = 10000
split_point = 3000

# Load and preprocess the datasets
X_train = load_images('/content/CIFAR10-mini/training', num_train_images, image_size)
#Y_train = load_labels('/content/drive/MyDrive/ColabNotebooks/CIFAR10-mini/training/labels.txt')
Y_train = load_labels('/content/CIFAR10-mini/training/labels.csv')
X_test = load_images('/content/CIFAR10-mini/testing', num_test_images, image_size)
Y_test = load_labels('/content/CIFAR10-mini/testing/labels.csv')

# Normalize the datasets
X_train = normalize_dataset(X_train)
X_test = normalize_dataset(X_test)

# Split the test data into validation and test sets
X_val, Y_val, X_test_remaining, Y_test_remaining = split_test_val(X_test, Y_test, split_point)

# One-hot encode the labels
Y_train_encoded = one_hot_encode_labels(Y_train, num_classes=10)
Y_val_encoded = one_hot_encode_labels(Y_val, num_classes=10)
Y_test_encoded = one_hot_encode_labels(Y_test_remaining, num_classes=10)

# Create and compile the model
input_shape = (32, 32, 3)
model = create_model(input_shape)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model using the loaded dataset for 12 epochs
history = model.fit(X_train, Y_train_encoded,
                    validation_data=(X_val, Y_val_encoded),
                    epochs=12,
                    batch_size=32)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_remaining, Y_test_encoded)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

#Assigment 4 start
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Define the ReduceLROnPlateau callback
reduce_lr = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.4,
    patience=3,
    verbose=1,
    min_delta=0.0001
)

# Train the model using 20 epochs and the ReduceLROnPlateau callback
history = model.fit(
    X_train, Y_train_encoded,
    validation_data=(X_val, Y_val_encoded),
    epochs=20,
    batch_size=32,
    callbacks=[reduce_lr]  # Include the callback
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_remaining, Y_test_encoded)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

#BatchNorminalization
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau # import ReduceLROnPlateau

# Define the ReduceLROnPlateau callback here
reduce_lr = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.4,
    patience=3,
    verbose=1,
    min_delta=0.0001
)

# Create the neural network model with BatchNormalization
def create_model_with_batch_norm(input_shape):
    x = Input(shape=input_shape)
    y = Conv2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(x)
    y = BatchNormalization()(y)  # Added BatchNormalization
    y = MaxPooling2D()(y)

    y = Conv2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = BatchNormalization()(y)  # Added BatchNormalization
    y = MaxPooling2D()(y)

    y = Conv2D(filters=128, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = BatchNormalization()(y)  # Added BatchNormalization

    y = Flatten()(y)
    y = Dense(128, activation='relu')(y)
    y = BatchNormalization()(y)  # Added BatchNormalization

    y = Dense(128, activation='relu')(y)
    y = BatchNormalization()(y)  # Added BatchNormalization

    y = Dense(10, activation='softmax')(y)  # No BatchNormalization here

    model = Model(inputs=x, outputs=y)
    return model

# Create and compile the model with BatchNormalization
model = create_model_with_batch_norm(input_shape)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model with the ReduceLROnPlateau callback
history_bn = model.fit(
    X_train, Y_train_encoded,
    validation_data=(X_val, Y_val_encoded),
    epochs=20,
    batch_size=32,
    callbacks=[reduce_lr]
)

# Evaluate the model
test_loss_bn, test_accuracy_bn = model.evaluate(X_test_remaining, Y_test_encoded)
print(f'Test Loss with BatchNormalization: {test_loss_bn}')
print(f'Test Accuracy with BatchNormalization: {test_accuracy_bn}')

!pip install tensorflow-addons
!pip install --upgrade tensorflow-addons
!pip install tensorflow==2.13.0
!pip install keras==2.13.1

#GroupNominalization
#from keras.src.layers.layer import GroupNormalization
#tf.keras.layers.GroupNormalization
from tensorflow_addons.layers import GroupNormalization

# Create the neural network model with GroupNormalization
def create_model_with_group_norm(input_shape, groups=8):
    x = Input(shape=input_shape)
    y = Conv2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(x)
    y = GroupNormalization(groups=groups)(y)  # Add GroupNormalization
    y = MaxPooling2D()(y)

    y = Conv2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = GroupNormalization(groups=groups)(y)  # Add GroupNormalization
    y = MaxPooling2D()(y)

    y = Conv2D(filters=128, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = GroupNormalization(groups=groups)(y)  # Add GroupNormalization

    y = Flatten()(y)
    y = Dense(128, activation='relu')(y)
    y = GroupNormalization(groups=groups)(y)  # Add GroupNormalization

    y = Dense(128, activation='relu')(y)
    y = GroupNormalization(groups=groups)(y)  # Add GroupNormalization

    y = Dense(10, activation='softmax')(y)  # No GroupNormalization here

    model = Model(inputs=x, outputs=y)
    return model

# Create and compile the model with GroupNormalization
model = create_model_with_group_norm(input_shape)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history_gn = model.fit(
    X_train, Y_train_encoded,
    validation_data=(X_val, Y_val_encoded),
    epochs=20,
    batch_size=32,
    callbacks=[reduce_lr]
)

# Evaluate the model
test_loss_gn, test_accuracy_gn = model.evaluate(X_test_remaining, Y_test_encoded)
print(f'Test Loss with GroupNormalization: {test_loss_gn}')
print(f'Test Accuracy with GroupNormalization: {test_accuracy_gn}')

from tensorflow.keras.layers import Dropout

# Create the neural network model with Dropout
def create_model_with_dropout(input_shape):
    x = Input(shape=input_shape)
    y = Conv2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(x)
    y = MaxPooling2D()(y)

    y = Conv2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)
    y = MaxPooling2D()(y)

    y = Conv2D(filters=128, kernel_size=(3, 3), strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(y)

    y = Flatten()(y)
    y = Dense(128, activation='relu')(y)
    y = Dropout(0.2)(y)  # Add Dropout layer after the 1st Dense layer

    y = Dense(128, activation='relu')(y)
    y = Dropout(0.2)(y)  # Add Dropout layer after the 2nd Dense layer

    y = Dense(10, activation='softmax')(y)  # No Dropout here

    model = Model(inputs=x, outputs=y)
    return model

# Create and compile the model with Dropout
model = create_model_with_dropout(input_shape)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model with Dropout layers
history_dropout = model.fit(
    X_train, Y_train_encoded,
    validation_data=(X_val, Y_val_encoded),
    epochs=20,
    batch_size=32,
    callbacks=[reduce_lr]
)

# Evaluate the model
test_loss_dropout, test_accuracy_dropout = model.evaluate(X_test_remaining, Y_test_encoded)
print(f'Test Loss with Dropout: {test_loss_dropout}')
print(f'Test Accuracy with Dropout: {test_accuracy_dropout}')

# Function to prepare the dataset
def make_dataset(x, y, batch_size, shuffle=False):
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    if shuffle:
        dataset = dataset.shuffle(buffer_size=1024)
    dataset = dataset.batch(batch_size)
    return dataset

# Create train and validation datasets
batch_size = 32
train_dataset = make_dataset(X_train, Y_train_encoded, batch_size=batch_size, shuffle=True)
val_dataset = make_dataset(X_val, Y_val_encoded, batch_size=batch_size)

# Loss and optimizer
loss_fn = tf.keras.losses.CategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)

# Metrics to track accuracy
train_acc_metric = tf.keras.metrics.CategoricalAccuracy()
val_acc_metric = tf.keras.metrics.CategoricalAccuracy()

# Custom training loop
epochs = 20
for epoch in range(epochs):
    print(f"\nStart of epoch {epoch + 1}")

    # Training phase
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        with tf.GradientTape() as tape:
            logits = model(x_batch_train, training=True)
            loss_value = loss_fn(y_batch_train, logits)

        grads = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        # Update training metric
        train_acc_metric.update_state(y_batch_train, logits)

        # Print loss every 100 steps
        if step % 100 == 0:
            print(f"Training loss (for one batch) at step {step}: {loss_value.numpy()}")

    # Display metrics at the end of an epoch
    train_acc = train_acc_metric.result()
    print(f"Training accuracy over epoch {epoch + 1}: {train_acc.numpy()}")
    train_acc_metric.reset_state()  # Changed from reset_states() to reset_state()

    # Validation phase
    for x_batch_val, y_batch_val in val_dataset:
        val_logits = model(x_batch_val, training=False)
        val_acc_metric.update_state(y_batch_val, val_logits)

    val_acc = val_acc_metric.result()
    print(f"Validation accuracy over epoch {epoch + 1}: {val_acc.numpy()}")
    val_acc_metric.reset_state()  # Changed from reset_states() to reset_state()

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_remaining, Y_test_encoded)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

# Question 5 (modified dataset function)
def make_dataset(x, y, batch_size, shuffle=False):
    """
    Create a tf.data.Dataset with optional shuffling and batching.

    Args:
        x (np.ndarray): Input data.
        y (np.ndarray): Target labels.
        batch_size (int): Batch size for the dataset.
        shuffle (bool): Whether to shuffle the dataset.

    Returns:
        tf.data.Dataset: Prepared dataset for training or evaluation.
    """
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    if shuffle:
        # Shuffle with a buffer size equal to the dataset size
        dataset = dataset.shuffle(buffer_size=len(x), reshuffle_each_iteration=True)
    dataset = dataset.batch(batch_size)
    return dataset
# Create train and validation datasets with shuffling for training
train_dataset = make_dataset(X_train, Y_train_encoded, batch_size=batch_size, shuffle=True)
val_dataset = make_dataset(X_val, Y_val_encoded, batch_size=batch_size)

# Loss and optimizer
loss_fn = tf.keras.losses.CategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)

# Metrics to track accuracy
train_acc_metric = tf.keras.metrics.CategoricalAccuracy()
val_acc_metric = tf.keras.metrics.CategoricalAccuracy()

# Custom training loop
epochs = 20
for epoch in range(epochs):
    print(f"\nStart of epoch {epoch + 1}")

    # Training phase
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        with tf.GradientTape() as tape:
            logits = model(x_batch_train, training=True)
            loss_value = loss_fn(y_batch_train, logits)

        grads = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        # Update training metric
        train_acc_metric.update_state(y_batch_train, logits)

        # Print loss every 100 steps
        if step % 100 == 0:
            print(f"Training loss (for one batch) at step {step}: {loss_value.numpy()}")

    # Display metrics at the end of an epoch
    train_acc = train_acc_metric.result()
    print(f"Training accuracy over epoch {epoch + 1}: {train_acc.numpy()}")
    train_acc_metric.reset_state()  # Changed from reset_states() to reset_state()

    # Validation phase
    for x_batch_val, y_batch_val in val_dataset:
        val_logits = model(x_batch_val, training=False)
        val_acc_metric.update_state(y_batch_val, val_logits)

    val_acc = val_acc_metric.result()
    print(f"Validation accuracy over epoch {epoch + 1}: {val_acc.numpy()}")
    val_acc_metric.reset_state()  # Changed from reset_states() to reset_state()

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_remaining, Y_test_encoded)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

tf.config.run_functions_eagerly(True)

def augment(images):
    """
    Apply data augmentation to a batch of images and ensure output dimensions match the model's input.

    Args:
        images (tf.Tensor): Batch of input images.

    Returns:
        tf.Tensor: Augmented images with the correct shape.
    """
    images = tf.image.random_flip_left_right(images)  # Random horizontal flip
    images = tf.image.random_flip_up_down(images)    # Random vertical flip
    images = tf.image.random_brightness(images, max_delta=0.1)  # Adjust brightness
    images = tf.image.random_saturation(images, lower=0.9, upper=1.1)  # Adjust saturation
    images = tf.image.random_hue(images, max_delta=0.1)  # Adjust hue
    images = tf.image.random_contrast(images, lower=0.9, upper=1.1)  # Adjust contrast

    # Optional: Crop and resize back to original dimensions
    images = tf.image.random_crop(images, size=[tf.shape(images)[0], 28, 28, 3])  # Crop to 28x28
    images = tf.image.resize(images, [32, 32])  # Resize back to 32x32

    # Ensure the output is a tensor
    return tf.convert_to_tensor(images)
    tf.print("Augmented batch shape:", tf.shape(images))

# Define the custom training loop with data augmentation
@tf.function
def train_one_epoch(model, optimizer, loss_fn, train_dataset, train_acc_metric):
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        # Apply data augmentation
        x_batch_train_augmented = augment(x_batch_train)

        with tf.GradientTape() as tape:
            # Forward pass
            logits = model(augment(x_batch_train), training=True)
            loss_value = loss_fn(y_batch_train, logits)

        # Backpropagation
        grads = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        # Update metrics
        train_acc_metric.update_state(y_batch_train, logits)

        if step % 100 == 0:
            print(f"Step {step}: loss = {loss_value.numpy():.4f}")

# Use the updated loop for training
for epoch in range(20):
    print(f"Epoch {epoch + 1}/{12}")
    train_one_epoch(model, optimizer, loss_fn, train_dataset, train_acc_metric)
    train_acc = train_acc_metric.result()
    print(f"Training accuracy over epoch: {train_acc:.4f}")
    train_acc_metric.reset_state() #

def cutmix(images, labels, alpha=1.0):
    """
    Apply CutMix augmentation to a batch of images and labels.

    Args:
        images (tf.Tensor): Batch of images (float32).
        labels (tf.Tensor): Corresponding labels (float32).
        alpha (float): CutMix alpha value for the Beta distribution.

    Returns:
        tf.Tensor, tf.Tensor: Augmented images and adjusted labels.
    """
    batch_size = tf.shape(images)[0]
    indices = tf.random.shuffle(tf.range(batch_size))  # Shuffle indices for mixing

    # Get shuffled images and labels
    shuffled_images = tf.gather(images, indices)
    shuffled_labels = tf.gather(labels, indices)

    # Sample lambda from Beta distribution
    lambda_val = np.random.beta(alpha, alpha)

    # Calculate the bounding box dimensions
    image_height, image_width = tf.shape(images)[1], tf.shape(images)[2]
    cut_rat = tf.sqrt(1.0 - lambda_val)

    # Cast image_height and image_width to float32 before multiplication to avoid type error
    cut_h = tf.cast(tf.round(tf.cast(image_height, tf.float32) * cut_rat), tf.int32)
    cut_w = tf.cast(tf.round(tf.cast(image_width, tf.float32) * cut_rat), tf.int32)

    # Randomly select the center of the bounding box
    cy = tf.random.uniform([], 0, image_height, dtype=tf.int32)
    cx = tf.random.uniform([], 0, image_width, dtype=tf.int32)

    # Define bounding box coordinates
    yl = tf.clip_by_value(cy - cut_h // 2, 0, image_height)
    yh = tf.clip_by_value(cy + cut_h // 2, 0, image_height)
    xl = tf.clip_by_value(cx - cut_w // 2, 0, image_width)
    xh = tf.clip_by_value(cx + cut_w // 2, 0, image_width)

    # Create binary mask with integers
    mask = tf.ones([yh - yl, xh - xl], dtype=tf.float32)
    pad_top = yl
    pad_bottom = image_height - yh
    pad_left = xl
    pad_right = image_width - xh

    # Pad the mask to match the image dimensions
    mask = tf.pad(mask, [[pad_top, pad_bottom], [pad_left, pad_right]], constant_values=0)
    mask = tf.expand_dims(mask, axis=-1)  # Make mask 3D to match image channels

    # Ensure images and mask are of the same data type
    mask = tf.cast(mask, tf.float32)
    images = tf.cast(images, tf.float32)
    shuffled_images = tf.cast(shuffled_images, tf.float32) # Specify the target data type (e.g., tf.float32)
    # Apply the mask to combine images
    images = images * (1 - mask) + shuffled_images * mask # Apply mask

    # Adjust labels based on lambda and one-hot encoding
    labels = labels * (1 - lambda_val) + shuffled_labels * lambda_val

    # Return the augmented images and adjusted labels
    return images, labels  # Add this line to return the values

for epoch in range(20):
    print(f"Epoch {epoch + 1}/{20}")

    # Training
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        with tf.GradientTape() as tape:
            # Apply CutMix augmentation
            x_batch_train, y_batch_train = cutmix(x_batch_train, y_batch_train)

            # Forward pass
            logits = model(x_batch_train, training=True)

            # Compute loss
            loss_value = loss_fn(y_batch_train, logits)

        # Backward pass
        grads = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        # Update metrics
        train_acc_metric.update_state(y_batch_train, logits)

    # Validation
    for x_batch_val, y_batch_val in val_dataset:
        val_logits = model(x_batch_val, training=False)
        val_acc_metric.update_state(y_batch_val, val_logits)

    # Display metrics
    train_acc = train_acc_metric.result()
    val_acc = val_acc_metric.result()
    print(f"Training accuracy: {train_acc:.4f}, Validation accuracy: {val_acc:.4f}")

    # Reset metrics
    train_acc_metric.reset_state()
    val_acc_metric.reset_state()

# Create an Adam optimizer with the specified learning rate
gc_adam_optimizer = adam(learning_rate=3e-4)

# Recompile the model with the Adam optimizer
model.compile(optimizer=gc_adam_optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Create an Adam optimizer with the specified learning rate
gc_adam_optimizer = adam(learning_rate=3e-4)

# Recompile the model with the Adam optimizer
model.compile(optimizer=gc_adam_optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, Y_train_encoded,
                    validation_data=(X_val, Y_val_encoded),
                    epochs=20,  # Adjust the epochs as needed
                    batch_size=32)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test_remaining, Y_test_encoded)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

# Import necessary libraries
import tensorflow as tf

# Define the add_noise function
def add_noise(grads):
    cgrads = []
    for grad in grads:
        rank = len(grad.shape)
        if rank > 1:
            rv = tf.random.normal(grad.shape, mean=0.0, stddev=1e-4, dtype=tf.dtypes.float32)
            grad = grad + rv
        cgrads.append(grad)
    return cgrads

# Define the add_grads function
def add_grads(parameters, gradients):
    new_grads = []
    for (params, grads) in zip(parameters, gradients):
        ap = 5e-1
        new_grads.append(grads + params)
    return new_grads

# Define the SADT fit function
def sadt_fit(train_dataset, val_dataset, model, optimizer, epochs):
    kld = tf.keras.losses.KLDivergence()

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        # Iterate over the batches of the dataset
        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
            with tf.GradientTape(persistent=True) as tape:
                logits = model(x_batch_train, training=True)
                loss_value = loss_fn(y_batch_train, logits)
            grads = tape.gradient(loss_value, model.trainable_weights)
            optimizer.apply_gradients(zip(grads, model.trainable_weights))

            # SADT: Perturb parameters and distill with auxiliary teacher
            wgt1 = model.get_weights()
            model.set_weights(add_noise(wgt1))

            with tf.GradientTape(persistent=True) as tape:
                logits1 = model(x_batch_train, training=True)
                loss_value = kld(logits, logits1)

            model.set_weights(wgt1)
            grads1 = tape.gradient(loss_value, model.trainable_weights)
            optimizer.apply_gradients(zip(add_grads(grads, grads1), model.trainable_weights))
            # SADT end

        # Display metrics at the end of each epoch
        val_loss, val_acc = model.evaluate(val_dataset, verbose=0)
        print(f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}")

    return model

# Load and prepare dataset
(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_val = x_val.astype("float32") / 255.0
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_val = tf.keras.utils.to_categorical(y_val, 10)

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Define optimizer and loss function
optimizer = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.CategoricalCrossentropy()

# Train the model
trained_model = sadt_fit(train_dataset, val_dataset, model, optimizer, epochs=10)

# Save the trained model
trained_model.save("sadt_model.h5")
print("Model saved as 'sadt_model.h5'")

import tensorflow as tf

def add_noise(weights, stddev=1e-4):
    """Add Gaussian noise to model weights."""
    noisy_weights = [w + tf.random.normal(w.shape, mean=0.0, stddev=stddev) for w in weights]
    return noisy_weights

def add_grads(grads, aux_grads, alpha=0.5):
    """Combine original and auxiliary gradients."""
    return [g + alpha * ag for g, ag in zip(grads, aux_grads)]

def sadt_fit(train_dataset, val_dataset, model, optimizer, epochs, loss_fn):
    """Structured Adversarial Data Training (SADT)."""
    kld = tf.keras.losses.KLDivergence()  # KL Divergence for distillation

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        epoch_loss = tf.keras.metrics.Mean()
        epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()

        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
            with tf.GradientTape(persistent=True) as tape:
                # Forward pass
                logits = model(x_batch_train, training=True)
                loss = loss_fn(y_batch_train, logits)

            # Compute gradients for the original model
            grads = tape.gradient(loss, model.trainable_weights)
            optimizer.apply_gradients(zip(grads, model.trainable_weights))

            # Perturb weights
            original_weights = model.get_weights()
            model.set_weights(add_noise(original_weights))

            # Compute auxiliary loss with perturbed weights
            with tf.GradientTape() as tape:
                perturbed_logits = model(x_batch_train, training=True)
                distillation_loss = kld(logits, perturbed_logits)

            # Restore original weights
            model.set_weights(original_weights)

            # Compute gradients for the auxiliary loss
            aux_grads = tape.gradient(distillation_loss, model.trainable_weights)
            combined_grads = add_grads(grads, aux_grads)

            # Apply combined gradients
            optimizer.apply_gradients(zip(combined_grads, model.trainable_weights))

            # Update metrics
            epoch_loss.update_state(loss)
            epoch_accuracy.update_state(y_batch_train, logits)

        print(f"Loss: {epoch_loss.result().numpy()}, Accuracy: {epoch_accuracy.result().numpy()}")

        # Validation metrics
        val_loss = tf.keras.metrics.Mean()
        val_accuracy = tf.keras.metrics.CategoricalAccuracy()
        for x_batch_val, y_batch_val in val_dataset:
            val_logits = model(x_batch_val, training=False)
            val_loss.update_state(loss_fn(y_batch_val, val_logits))
            val_accuracy.update_state(y_batch_val, val_logits)
        print(f"Validation Loss: {val_loss.result().numpy()}, Validation Accuracy: {val_accuracy.result().numpy()}")

    return model

import tensorflow as tf
from tensorflow.keras import layers, models

def create_model(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax'),
    ])
    return model

# Create the model
model = create_model()

# Define optimizer and loss function
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
loss_fn = tf.keras.losses.CategoricalCrossentropy()

# Prepare datasets
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Create tf.data datasets
batch_size = 64
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)
val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)



# Train the model using SADT
trained_model = sadt_fit(train_dataset, val_dataset, model, optimizer, epochs=20, loss_fn=loss_fn)

model.summary()

##Assigment 2 start
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from PIL import Image
# Function to load images
def load_images(folder, image_count, image_size):
    array_shape = (image_count, image_size[0], image_size[1], image_size[2])  # Original shape: B✕H✕W✕C
    imageset = np.empty(array_shape, dtype='float32')
    for i in range(image_count):
        image = Image.open(f"{folder}/image_{i:04d}.png").resize((image_size[1], image_size[0]))  # Ensure resizing to target H, W
        imageset[i] = np.asarray(image) / 255.0  # Normalize to [0,1]
    # Transpose to PyTorch format (B✕C✕H✕W)
    imageset = np.transpose(imageset, (0, 3, 1, 2))  # Transpose to (batch, channels, height, width)
    return imageset
# Function to load labels
def load_labels(filename):
    with open(filename, 'r') as file:
        lines = file.readlines()
    labels = np.array([int(line.strip()) for line in lines], dtype='int')
    return labels
# Function to normalize images to the range [-1, 1]
def normalize_dataset(images):
    return images * 2.0 - 1.0  # Scale to [-1, 1] range
def one_hot_encode_labels(labels, num_classes=10):
    return to_categorical(labels, num_classes=num_classes)
# Function to split data into test and validation sets
def split_test_val(data, split_point):
    return data[:split_point], data[split_point:]

# Assuming load_images, load_labels, normalize_dataset, and split_test_val are already defined
# Load and preprocess the data
Y_test = load_labels('testing/labels.csv')
X_test = load_images('testing', len(Y_test), (32,32,3))
X_test = normalize_dataset(X_test)
y_train = load_labels('training/labels.csv')
x_train = load_images('training', len(y_train), (32,32,3))
x_train = normalize_dataset(x_train)
# Split test data into test and validation sets
split_point = 3000
x_test, x_val = split_test_val(X_test, split_point)
y_test, y_val = split_test_val(Y_test, split_point)
#create one hot labels
y_train = one_hot_encode_labels(y_train, num_classes=10)
y_val = one_hot_encode_labels(y_val, num_classes=10)
y_test = one_hot_encode_labels(y_test, num_classes=10)
# Convert data arrays to PyTorch tensors
train_images_tensor = torch.Tensor(x_train)  # Training images tensor
train_labels_tensor = torch.Tensor(y_train).long()  # Training labels tensor
val_images_tensor = torch.Tensor(x_val)  # Validation images tensor
val_labels_tensor = torch.Tensor(y_val).long()  # Validation labels tensor
test_images_tensor = torch.Tensor(x_test)  # Test images tensor
test_labels_tensor = torch.Tensor(y_test).long()  # Test labels tensor
# Create TensorDatasets
train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)
val_dataset = TensorDataset(val_images_tensor, val_labels_tensor)
test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)
# Define DataLoaders with appropriate batch sizes and shuffling options
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

from torch.utils.data import DataLoader, TensorDataset

# Assuming x_val, y_val, x_test, and y_test are the validation and test datasets
# Convert numpy arrays to PyTorch tensors if they aren't already
x_val_tensor = torch.Tensor(x_val).to("cuda")
y_val_tensor = torch.LongTensor(y_val).to("cuda")

x_test_tensor = torch.Tensor(x_test).to("cuda")
y_test_tensor = torch.LongTensor(y_test).to("cuda")

# Create TensorDataset and DataLoader for validation and test sets
validation_dataset = TensorDataset(x_val_tensor, y_val_tensor)
validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)

test_dataset = TensorDataset(x_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Now, you can run the validation/test loop using validation_loader or test_loader as needed.

import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding='same')  # Input channels=3, Output channels=32
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding='same')  # Input=32, Output=64
        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding='same')  # Input=64, Output=128
        # Fully connected (Linear) layers
        self.fc1 = nn.Linear(128 * 4 * 4, 128)  # 128 feature maps with 4x4 each
        self.fc2 = nn.Linear(128, 128)  # Corrected Output layer
        self.output = nn.Linear(128, 10)  # Final output layer with 10 classes (e.g., CIFAR-10)

    def forward(self, x):
        # First convolutional layer
        x = self.conv1(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        # Second convolutional layer
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        # Third convolutional layer
        x = self.conv3(x)
        x = F.relu(x)
        # Flattening the output for the fully connected layers
        x = torch.flatten(x, 1)
        # First fully connected layer
        x = self.fc1(x)
        x = F.relu(x)
        # Corrected fully connected layer
        x = self.fc2(x)
        x = F.relu(x)
        # Output layer
        output = self.output(x)
        output = F.log_softmax(output, dim=1)  # Log softmax for classification
        return output

# Initialize the model and move it to GPU
my_nn = Net().to("cuda")
from torchsummary import summary
summary(my_nn, (3, 32, 32))
# Define the loss function
loss_fn = nn.CrossEntropyLoss()
# Define the optimizer
optimizer = optim.Adam(my_nn.parameters(), lr=2e-4)
# Training loop
num_epochs = 50  # Adjust as needed
for epoch in range(num_epochs):
    for x_batch, y_batch in train_loader:  # train_loader should already be defined
        # Move data to GPU
        x_batch = x_batch.to("cuda")
        y_batch = y_batch.to("cuda")
        # Convert one-hot encoded labels to class indices
        y_batch = torch.argmax(y_batch, dim=1) #This line is added to convert one-hot to class indices
        # Forward pass
        y_pred = my_nn(x_batch)
        loss = loss_fn(y_pred, y_batch)  # Compute the loss
        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    # Optional: Print loss for tracking progress
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}")

# Validation/Test loop
correct_labels = 0
total_label_count = 0

# Set the model to evaluation mode
my_nn.eval()

# Disable gradient computation for validation/test
with torch.no_grad():
    for x_batch, y_batch in validation_loader:  # Replace 'validation_loader' with 'test_loader' for testing
        # Move data to GPU
        x_batch = x_batch.to("cuda")
        y_batch = y_batch.to("cuda")

        # Forward pass
        validation_outputs = my_nn(x_batch)

        # Get predictions
        _, predicted_labels = torch.max(validation_outputs.data, 1)

        # Convert one-hot encoded labels to class indices before comparison
        actual_labels = torch.argmax(y_batch, dim=1)

        # Update the total and correct label counts
        total_label_count += y_batch.size(0)
        correct_labels += (predicted_labels == actual_labels).sum().item() # Compare with actual_labels

# Calculate accuracy
accuracy = (correct_labels / total_label_count) * 100
print(f"Validation/Test Accuracy: {accuracy:.2f}%")

# Training loop
num_epochs = 50  # Adjust as needed
for epoch in range(num_epochs):
    my_nn.train()  # Set the model to training mode
    running_loss = 0.0

    for x_batch, y_batch in train_loader:
        # Move data to GPU
        x_batch = x_batch.to("cuda")
        y_batch = y_batch.to("cuda")

        # Convert one-hot encoded labels to class indices
        y_batch = torch.argmax(y_batch, dim=1) #This line is added to convert one-hot to class indices

        # Forward pass
        outputs = my_nn(x_batch)
        loss = loss_fn(outputs, y_batch)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Accumulate loss
        running_loss += loss.item()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}")

# Validation loop
correct_labels = 0
total_label_count = 0

# Set the model to evaluation mode
my_nn.eval()

with torch.no_grad():
    for x_batch, y_batch in validation_loader:
        # Move data to GPU
        x_batch = x_batch.to("cuda")
        y_batch = y_batch.to("cuda")

        # Forward pass
        outputs = my_nn(x_batch)

        # Get predictions
        _, predicted_labels = torch.max(outputs, 1)

        # Convert one-hot encoded labels to class indices before comparison
        actual_labels = torch.argmax(y_batch, dim=1)

        # Update total and correct counts
        total_label_count += y_batch.size(0)
        correct_labels += (predicted_labels == actual_labels).sum().item() # Compare with actual_labels

validation_accuracy = (correct_labels / total_label_count) * 100
print(f"Validation Accuracy: {validation_accuracy:.2f}%")

# Testing loop
correct_labels = 0
total_label_count = 0

# Set the model to evaluation mode
my_nn.eval()

with torch.no_grad():
    for x_batch, y_batch in test_loader:
        # Move data to GPU
        x_batch = x_batch.to("cuda")
        y_batch = y_batch.to("cuda")

        # Forward pass
        outputs = my_nn(x_batch)

        # Get predictions
        _, predicted_labels = torch.max(outputs, 1)

        # Convert one-hot encoded labels to class indices before comparison
        actual_labels = torch.argmax(y_batch, dim=1) # Convert y_batch to class indices

        # Update total and correct counts
        total_label_count += y_batch.size(0)
        correct_labels += (predicted_labels == actual_labels).sum().item() # Compare with actual_labels

test_accuracy = (correct_labels / total_label_count) * 100
print(f"Test Accuracy: {test_accuracy:.2f}%")